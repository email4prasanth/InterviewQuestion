# Centralized Kafka Deployment with Cross-VM Communication

## 1. Context and Challenge

The client had multiple VMs running backend and ETL services. Initially, Kafka was deployed only on the **Backend VM**, but both Backend and ETL required access. Challenges included:

- **Cross-VM access**: Kafka needed to be reliably available to multiple VMs.  
- **Service startup issues**: Modules failed to connect after VM restarts.  
- **Monitoring gap**: No end-to-end visibility into message lag, processed counts, or errors.  
- **Operational efficiency**: Needed centralized deployment to simplify maintenance and troubleshooting.  

---

## 2. Technical Complexity

The task involved migrating and centralizing Kafka while ensuring reliability and observability:

- **Kafka + Zookeeper migration**: Moved from Backend VM to Redis VM using Docker Compose.  
- **Cross-VM Docker networking**: Configured shared Docker network to allow Backend and ETL VMs to communicate with Kafka.  
- **Service reliability**: Created systemd service with delayed start and health checks to handle dependency on Kafka readiness after VM restarts.  
- **Monitoring and logging**:
  - Cron-based script (`/usr/local/bin/kafka_monitor.sh`) to track topic offsets, message lag, and DLQ errors.  
  - Logs pushed to **syslog (local4)** and integrated with **Azure Log Analytics** for dashboards and queries.  
- **Validation**: Tested producing/consuming messages locally and remotely, confirmed metrics collection and dashboard visibility.  

---

## 3. Client and Team Requirements

- Provide **centralized Kafka deployment** accessible by multiple VMs.  
- Ensure **reliable service startup** and minimal downtime after VM restarts.  
- Enable **end-to-end monitoring** for topics, lag, and errors.  
- Maintain **operational visibility** and troubleshooting capabilities via logs and dashboards.  

---

## 4. Solution Approach

- Migrated Kafka + Zookeeper to Redis VM using Docker Compose.  
- Configured **shared Docker network** for Backend and ETL communication.  
- Automated module startup with **systemd delayed restart** and container **health checks**.  
- Implemented **monitoring script** for Kafka topics with cron scheduling, integrated logs into **syslog and Azure Log Analytics**.  
- Verified connectivity and metrics by testing Kafka CLI tools on Backend, ETL, and Redis VMs.  

---

## 5. Outcome / Result

- Kafka is now **centralized and stable** on Redis VM.  
- **Cross-VM communication** works reliably via Docker network.  
- Startup dependency issues resolved with **systemd delayed restart**.  
- End-to-end **monitoring and observability** implemented with metrics, alerts, and dashboards.  
- Simplified maintenance and troubleshooting for Backend and ETL teams.  

---

## Concise, Resume-Friendly Markdown Version

### DevOps Engineer | Project: Centralized Kafka Deployment

- Centralized Kafka and Zookeeper on a dedicated VM using **Docker Compose** and **shared Docker networks** for cross-VM access.  
- Automated module startup with **systemd delayed restarts** and container health checks to resolve post-reboot connectivity issues.  
- Implemented **end-to-end Kafka monitoring** via cron-driven scripts, logging to syslog, and integration with Azure Log Analytics for topic offsets, lag, and DLQ errors.  
- Validated **reliable cross-VM communication** and operational observability for Backend and ETL services.  
- Delivered **stable, maintainable, and fully monitored Kafka deployment** with minimal operational overhead.  

---

## Resume One Line

- Centralized Kafka on a VM with Docker, ensuring reliable cross-VM communication, automated service restarts, and real-time monitoring of message processing and errors.  

---

## 30 Seconds Version

In one project, I **centralized Kafka and Zookeeper on a dedicated VM** to serve Backend and ETL services. I used **Docker Compose with a shared network** for cross-VM access and automated module startup using **systemd delayed restarts** and container health checks. I built a **cron-based monitoring script** to track message lag, offsets, and DLQ errors, pushing logs to **syslog and Azure Log Analytics**. This gave us a **stable, observable Kafka deployment** with reliable service connectivity and streamlined operations.
